{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03ad7f4",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44037170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d61952",
   "metadata": {},
   "source": [
    "Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9125b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n",
      "Shape:  (1048575, 4)\n",
      "Sentence #    1000616\n",
      "Word               10\n",
      "POS                 0\n",
      "Tag                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/ner_dataset.csv', encoding = 'latin1')\n",
    "print(df.head())\n",
    "print(\"Shape: \",df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a0d00",
   "metadata": {},
   "source": [
    "Fix Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c279bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAzY BUGzzz\\AppData\\Local\\Temp\\ipykernel_3108\\3084705801.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Sentence #'].ffill(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #    0\n",
      "Word          0\n",
      "POS           0\n",
      "Tag           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['Sentence #'].ffill(inplace=True)\n",
    "df = df.dropna(subset=['Word'])\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79a5c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentences: 47959\n",
      "Entity tags: ['O' 'B-geo' 'B-gpe' 'B-per' 'I-geo' 'B-org' 'I-org' 'B-tim' 'B-art'\n",
      " 'I-art' 'I-per' 'I-gpe' 'I-tim' 'B-nat' 'B-eve' 'I-eve' 'I-nat']\n",
      "Tag\n",
      "O        887898\n",
      "B-geo     37644\n",
      "B-tim     20333\n",
      "B-org     20143\n",
      "I-per     17251\n",
      "B-per     16990\n",
      "I-org     16784\n",
      "B-gpe     15870\n",
      "I-geo      7414\n",
      "I-tim      6528\n",
      "B-art       402\n",
      "B-eve       308\n",
      "I-art       297\n",
      "I-eve       253\n",
      "B-nat       201\n",
      "I-gpe       198\n",
      "I-nat        51\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique sentences:\", df[\"Sentence #\"].nunique())\n",
    "print(\"Entity tags:\", df[\"Tag\"].unique())\n",
    "print(df[\"Tag\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b71b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: ['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
      "Example labels: ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "labels = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).tolist()\n",
    "\n",
    "print(\"Example sentence:\", sentences[0])\n",
    "print(\"Example labels:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391cb61",
   "metadata": {},
   "source": [
    "Rule Based NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff1142",
   "metadata": {},
   "source": [
    "Test on first 5 rows to check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5304a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Rule matched: London --> LOCATION\n",
      "\n",
      "Sentence: Iranian officials say they expect to get access to sealed sensitive parts of the plant Wednesday , after an IAEA surveillance system begins functioning .\n",
      "Rule matched: Wednesday --> DAY/MONTH\n",
      "\n",
      "Sentence: Helicopter gunships Saturday pounded militant hideouts in the Orakzai tribal region , where many Taliban militants are believed to have fled to avoid an earlier military offensive in nearby South Waziristan .\n",
      "Rule matched: Saturday --> DAY/MONTH\n",
      "\n",
      "Sentence: They left after a tense hour-long standoff with riot police .\n",
      "\n",
      "Sentence: U.N. relief coordinator Jan Egeland said Sunday , U.S. , Indonesian and Australian military helicopters are ferrying out food and supplies to remote areas of western Aceh province that ground crews can not reach .\n",
      "Rule matched: Sunday --> DAY/MONTH\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Rules\n",
    "# Organization\n",
    "pattern_org = [{\"TEXT\": {\"REGEX\": \".*(Inc\\\\.|Ltd\\\\.|Corp\\\\.|University)$\"}}]\n",
    "matcher.add(\"ORGANIZATION\", [pattern_org])\n",
    "\n",
    "# Person\n",
    "pattern_person = [{\"TEXT\": {\"REGEX\": \"(Mr\\\\.|Mrs\\\\.|Dr\\\\.|Prof\\\\.)\"}}, {\"IS_TITLE\": True}]\n",
    "matcher.add(\"PERSON\", [pattern_person])\n",
    "\n",
    "# Countires/Cities\n",
    "pattern_gpe = [{\"LOWER\": {\"IN\": [\"pakistan\", \"india\", \"china\", \"london\", \"usa\", \"france\"]}}]\n",
    "matcher.add(\"LOCATION\", [pattern_gpe])\n",
    "\n",
    "# Day\n",
    "pattern_day = [{\"LOWER\": {\"IN\": [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]}}]\n",
    "matcher.add(\"DAY/MONTH\", [pattern_day])\n",
    "\n",
    "# Year\n",
    "pattern_year = [{\"SHAPE\": \"dddd\"}]  \n",
    "matcher.add(\"YEAR\", [pattern_year])\n",
    "\n",
    "# Time\n",
    "pattern_time = [{\"TEXT\": {\"REGEX\": \"^(\\\\d{1,2}:\\\\d{2}|\\\\d{1,2}(am|pm|AM|PM))$\"}}]\n",
    "matcher.add(\"TIME\", [pattern_time])\n",
    "\n",
    "# Event\n",
    "pattern_event = [{\"LOWER\": {\"IN\": [\"olympics\",\"summit\",\"world cup\",\"conference\"]}}]\n",
    "matcher.add(\"EVENT\", [pattern_event])\n",
    "\n",
    "# Title in Quotes (artwork)\n",
    "pattern_art = [{\"TEXT\": {\"REGEX\": \"^[\\\"“].+[\\\"”]$\"}}]\n",
    "matcher.add(\"ART\", [pattern_art])\n",
    "\n",
    "# Nationalities/Religions/Political groups\n",
    "pattern_norp = [{\"LOWER\": {\"IN\": [\"muslim\",\"christian\",\"hindu\",\"republican\",\"democrat\",\"asian\",\"european\"]}}]\n",
    "matcher.add(\"NATIONALITY/RELIGION/POLITICAL GROUP\", [pattern_norp])\n",
    "\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "sample_sentences = [\" \".join(s) for s in sentences[:5]]   \n",
    "\n",
    "for sent in sample_sentences:\n",
    "    doc = nlp(sent)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"\\nSentence: {sent}\")\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        print(f\"Rule matched: {span.text} --> {nlp.vocab.strings[match_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eec6de",
   "metadata": {},
   "source": [
    "Apply Rule based NER on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2132c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Rules\n",
    "# Organization\n",
    "pattern_org = [{\"TEXT\": {\"REGEX\": \".*(Inc\\\\.|Ltd\\\\.|Corp\\\\.|University)$\"}}]\n",
    "matcher.add(\"ORGANIZATION\", [pattern_org])\n",
    "\n",
    "# Person\n",
    "pattern_person = [{\"TEXT\": {\"REGEX\": \"(Mr\\\\.|Mrs\\\\.|Dr\\\\.|Prof\\\\.)\"}}, {\"IS_TITLE\": True}]\n",
    "matcher.add(\"PERSON\", [pattern_person])\n",
    "\n",
    "# Countires/Cities\n",
    "pattern_gpe = [{\"LOWER\": {\"IN\": [\"pakistan\", \"india\", \"china\", \"london\", \"usa\", \"france\"]}}]\n",
    "matcher.add(\"LOCATION\", [pattern_gpe])\n",
    "\n",
    "# Day\n",
    "pattern_day = [{\"LOWER\": {\"IN\": [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]}}]\n",
    "matcher.add(\"DAY/MONTH\", [pattern_day])\n",
    "\n",
    "# Year\n",
    "pattern_year = [{\"SHAPE\": \"dddd\"}]  \n",
    "matcher.add(\"YEAR\", [pattern_year])\n",
    "\n",
    "# Time\n",
    "pattern_time = [{\"TEXT\": {\"REGEX\": \"^(\\\\d{1,2}:\\\\d{2}|\\\\d{1,2}(am|pm|AM|PM))$\"}}]\n",
    "matcher.add(\"TIME\", [pattern_time])\n",
    "\n",
    "# Event\n",
    "pattern_event = [{\"LOWER\": {\"IN\": [\"olympics\",\"summit\",\"world cup\",\"conference\"]}}]\n",
    "matcher.add(\"EVENT\", [pattern_event])\n",
    "\n",
    "# Title in Quotes (artwork)\n",
    "pattern_art = [{\"TEXT\": {\"REGEX\": \"^[\\\"“].+[\\\"”]$\"}}]\n",
    "matcher.add(\"ART\", [pattern_art])\n",
    "\n",
    "# Nationalities/Religions/Political groups\n",
    "pattern_norp = [{\"LOWER\": {\"IN\": [\"muslim\",\"christian\",\"hindu\",\"republican\",\"democrat\",\"asian\",\"european\"]}}]\n",
    "matcher.add(\"NATIONALITY/RELIGION/POLITICAL GROUP\", [pattern_norp])\n",
    "\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "all_sentences = [\" \".join(s) for s in sentences]\n",
    "\n",
    "# Apply Rules and save output to seperate file\n",
    "with open(\"OutPut_Files/rule_based_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in all_sentences:\n",
    "        doc = nlp(sent)\n",
    "        matches = matcher(doc)\n",
    "        f.write(f\"\\nSentence: {sent}\\n\")\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            f.write(f\"Rule matched: {span.text} --> {nlp.vocab.strings[match_id]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2a2a5",
   "metadata": {},
   "source": [
    "Model Based NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f19291",
   "metadata": {},
   "source": [
    "On first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Model detected: Thousands --> CARDINAL\n",
      "Model detected: London --> GPE\n",
      "Model detected: Iraq --> GPE\n",
      "Model detected: British --> NORP\n",
      "\n",
      "Sentence: Iranian officials say they expect to get access to sealed sensitive parts of the plant Wednesday , after an IAEA surveillance system begins functioning .\n",
      "Model detected: Iranian --> NORP\n",
      "Model detected: Wednesday --> DATE\n",
      "Model detected: IAEA --> ORG\n",
      "\n",
      "Sentence: Helicopter gunships Saturday pounded militant hideouts in the Orakzai tribal region , where many Taliban militants are believed to have fled to avoid an earlier military offensive in nearby South Waziristan .\n",
      "Model detected: Saturday --> DATE\n",
      "Model detected: Orakzai --> PERSON\n",
      "Model detected: Taliban --> ORG\n",
      "Model detected: South Waziristan --> GPE\n",
      "\n",
      "Sentence: They left after a tense hour-long standoff with riot police .\n",
      "Model detected: hour --> TIME\n",
      "\n",
      "Sentence: U.N. relief coordinator Jan Egeland said Sunday , U.S. , Indonesian and Australian military helicopters are ferrying out food and supplies to remote areas of western Aceh province that ground crews can not reach .\n",
      "Model detected: U.N. --> ORG\n",
      "Model detected: Jan Egeland --> PERSON\n",
      "Model detected: Sunday --> DATE\n",
      "Model detected: U.S. --> GPE\n",
      "Model detected: Indonesian --> NORP\n",
      "Model detected: Australian --> NORP\n",
      "Model detected: western Aceh --> GPE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "sample_sentences = [\" \".join(s) for s in sentences[:5]] \n",
    "\n",
    "for sent in sample_sentences:\n",
    "    doc = nlp(sent)\n",
    "    print(f\"\\nSentence: {sent}\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Model detected: {ent.text} --> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e2d74",
   "metadata": {},
   "source": [
    "Model based NER on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187cdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "all_sentences = [\" \".join(s) for s in sentences]\n",
    "\n",
    "# Apply model and save output\n",
    "with open(\"OutPut_Files/model_based_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in all_sentences:\n",
    "        doc = nlp(sent)\n",
    "        f.write(f\"\\nSentence: {sent}\\n\")\n",
    "        for ent in doc.ents:\n",
    "            f.write(f\"Model detected: {ent.text} --> {ent.label_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ac1c5",
   "metadata": {},
   "source": [
    "Model Based NER - HTML Content for entities Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3c4214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\Elevvo Tech\\NLP_Projects\\Named_Entity_Recognition\\venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sentences visualized! Open 'model_based_output_ALL.html' in your browser.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "all_sentences = [\" \".join(s) for s in sentences]\n",
    "\n",
    "\n",
    "html_content = \"\"\n",
    "for sent in all_sentences:\n",
    "    doc = nlp(sent)\n",
    "    html_content += displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "    html_content += \"<hr>\"\n",
    "\n",
    "with open(\"OutPut_Files/model_based_output_ALL.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>NER Visualization - Entire Dataset</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .sentence {{ margin-bottom: 30px; border-bottom: 2px solid #ccc; padding-bottom: 20px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Named Entity Recognition - Entire Dataset</h1>\n",
    "        {html_content}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "print(\"All sentences visualized! Open 'model_based_output_ALL.html' in your browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8844f",
   "metadata": {},
   "source": [
    "Rule Based NER - HTML Content for entities Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb28fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\Elevvo Tech\\NLP_Projects\\Named_Entity_Recognition\\venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule Based NER visualization complete! Open 'rule_based_output_100.html' in your browser.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Rules\n",
    "pattern_org = [{\"TEXT\": {\"REGEX\": \".*(Inc\\\\.|Ltd\\\\.|Corp\\\\.|University)$\"}}]\n",
    "matcher.add(\"ORGANIZATION\", [pattern_org])\n",
    "\n",
    "pattern_person = [{\"TEXT\": {\"REGEX\": \"(Mr\\\\.|Mrs\\\\.|Dr\\\\.|Prof\\\\.)\"}}, {\"IS_TITLE\": True}]\n",
    "matcher.add(\"PERSON\", [pattern_person])\n",
    "\n",
    "pattern_gpe = [{\"LOWER\": {\"IN\": [\"pakistan\", \"india\", \"china\", \"london\", \"usa\", \"france\"]}}]\n",
    "matcher.add(\"LOCATION\", [pattern_gpe])\n",
    "\n",
    "pattern_day = [{\"LOWER\": {\"IN\": [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]}}]\n",
    "matcher.add(\"DAY/MONTH\", [pattern_day])\n",
    "\n",
    "pattern_year = [{\"SHAPE\": \"dddd\"}]  \n",
    "matcher.add(\"YEAR\", [pattern_year])\n",
    "\n",
    "pattern_time = [{\"TEXT\": {\"REGEX\": \"^(\\\\d{1,2}:\\\\d{2}|\\\\d{1,2}(am|pm|AM|PM))$\"}}]\n",
    "matcher.add(\"TIME\", [pattern_time])\n",
    "\n",
    "pattern_event = [{\"LOWER\": {\"IN\": [\"olympics\",\"summit\",\"world cup\",\"conference\"]}}]\n",
    "matcher.add(\"EVENT\", [pattern_event])\n",
    "\n",
    "pattern_art = [{\"TEXT\": {\"REGEX\": \"^[\\\"\\\"].+[\\\"\\\"]$\"}}]\n",
    "matcher.add(\"ART\", [pattern_art])\n",
    "\n",
    "pattern_norp = [{\"LOWER\": {\"IN\": [\"muslim\",\"christian\",\"hindu\",\"republican\",\"democrat\",\"asian\",\"european\"]}}]\n",
    "matcher.add(\"NATIONALITY/RELIGION/POLITICAL GROUP\", [pattern_norp])\n",
    "\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "all_sentences = [\" \".join(s) for s in sentences]\n",
    "\n",
    "# HTML content \n",
    "html_content = \"\"\n",
    "for i, sent in enumerate(all_sentences[:100]):  #First 100 sentences\n",
    "    doc = nlp(sent)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = Span(doc, start, end, label=nlp.vocab.strings[match_id])\n",
    "        spans.append(span)\n",
    "    doc.ents = spans\n",
    "    \n",
    "    html_content += f\"<h3>Sentence {i+1}:</h3>\"\n",
    "    html_content += displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "    html_content += \"<hr>\"\n",
    "\n",
    "with open(\"OutPut_Files/rule_based_output_100.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Rule Based NER Visualization - First 100 Sentences</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Rule Based Named Entity Recognition - First 100 Sentences</h1>\n",
    "        {html_content}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Rule Based NER visualization complete! Open 'rule_based_output_100.html' in your browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b3ccf",
   "metadata": {},
   "source": [
    "Spacy Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e32660",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
    "all_sentences = [\" \".join(s) for s in sentences]\n",
    "\n",
    "with open(\"OutPut_Files/model_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in all_sentences[:500]:  #for 500 sentences\n",
    "        doc_sm = nlp_sm(sent)\n",
    "        doc_md = nlp_md(sent)\n",
    "\n",
    "        f.write(f\"\\nSentence: {sent}\\n\")\n",
    "        f.write(\"SM Model Entities:\\n\")\n",
    "        for ent in doc_sm.ents:\n",
    "            f.write(f\"   {ent.text} --> {ent.label_}\\n\")\n",
    "\n",
    "        f.write(\"MD Model Entities:\\n\")\n",
    "        for ent in doc_md.ents:\n",
    "            f.write(f\"   {ent.text} --> {ent.label_}\\n\")\n",
    "        sm_count = sum(1 for sent in all_sentences[:500] for ent in nlp_sm(sent).ents)\n",
    "        md_count = sum(1 for sent in all_sentences[:500] for ent in nlp_md(sent).ents)\n",
    "    f.write(f\"\\nSUMMARY: en_core_web_md detected {md_count} entities vs en_core_web_sm detected {sm_count} entities - {'MD model performs better' if md_count > sm_count else 'SM model performs better' if sm_count > md_count else 'Both models detected same number of entities'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
